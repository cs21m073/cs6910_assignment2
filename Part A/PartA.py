# -*- coding: utf-8 -*-
"""PartA_Last.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1auA7CSnkEHRDes5R5g7c4v4FkVy1ESxP
"""

import numpy as np 
import os
import tensorflow as tf
import matplotlib.image as mpimg
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow import keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
import cv2


##Creating the dataset from google drive
Path = '/content/drive/MyDrive/inaturalist_12K'

trainPath = os.path.join(Path, "train") #training data path
valPath = os.path.join(Path, "val") #validation data path

def createDataset(augmentData=False):
  if augmentData == True:
    train_ = ImageDataGenerator(rescale=1./255, zoom_range=0.2, shear_range=0.2, rotation_range=90, horizontal_flip=True, validation_split=0.1)
  else:
    train_ = ImageDataGenerator(rescale=1./255, validation_split=0.1)
  
  val_ = ImageDataGenerator(rescale=1./255)

  trainData = train_.flow_from_directory(trainPath, target_size=(227, 227), batch_size=32, subset="training", shuffle =True, seed = 777)
  testData = train_.flow_from_directory(trainPath, target_size=(227, 227), batch_size=32, subset="validation", shuffle = True, seed = 777)
  valData = val_.flow_from_directory(valPath, target_size=(227, 227), batch_size=32)

  return trainData, testData, valData

  


class ConvolutionClasss:
  def __init__(self):
    pass

  def CNNModel(self, numOfFilters=32, filterMultiplier=1, denseSize=64, imageSize=200, dropout=0.2, batchNorm=False, numClasses=10):
    model = Sequential()
    filterDim1 = 11
    for i in range(5):
        filterDim = filterDim1 - 2*i
        filterSize = (filterDim, filterDim)
        if i==0:
            model.add(Conv2D(numOfFilters, filterSize, input_shape=(imageSize, imageSize, 3), data_format="channels_last"))
        else:
            model.add(Conv2D(numOfFilters, filterSize))
        if batchNorm:
            model.add(BatchNormalization())
        model.add(Activation("relu"))
        model.add(MaxPooling2D(pool_size=(2,2)))
        numOfFilters = int(numOfFilters * filterMultiplier)
    
    model.add(Flatten())
    model.add(Dense(denseSize))
    model.add(Dropout(dropout))
    model.add(Activation("relu"))
    model.add(Dense(numClasses))
    model.add(Activation("softmax"))
    return model


    #Training the model
  def trainModel(self, numOfFilters, filterMultiplier, denseSize, dropout, batchNorm, lr, epochs, augment_data):

        trainData, testData, valData = createDataset(augment_data)

        #structure of CNN(numOfFilters=32, filterMultiplier=1, denseSize=64, imageSize=200, dropout=0.2, batchNorm=False, numClasses=10)
        model = self.CNNModel(numOfFilters, filterMultiplier, denseSize, 227, dropout, batchNorm)

        model.compile(optimizer=keras.optimizers.Adam(lr), loss="categorical_crossentropy", metrics="categorical_accuracy")
        model.fit(trainData, epochs=epochs, validation_data=testData)
        #model.save("/content/drive/MyDrive/inaturalist_12K/model.h5")
        model.evaluate(valData)



from sys import argv
if __name__== "__main__":
  obj = ConvolutionClasss()
  numOfFilters = int(argv[1])
  filterMultiplier = int(argv[2])
  denseSize = int(argv[3])
  dropout = float(argv[4])
  
  if argv[5] == "True":
  	batchNorm = True
  else:
  	batchNorm = False
  lr = float(argv[6])
  
  if argv[7] == "True":
  	augment_data = True
  else:
  	augment_data = False
  	
  epochs = int(argv[8])
  obj.trainModel(numOfFilters, filterMultiplier, denseSize, dropout, batchNorm, lr, augment_data, epochs)
